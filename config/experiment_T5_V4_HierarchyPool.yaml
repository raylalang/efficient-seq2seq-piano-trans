defaults:
  - experiment_T5_V3_HybridGlobalLocalCrossAttn

model:
  checkpoint_path: # "checkpoints/T5_V4_steps_200000.ckpt"

  cross_attention_hierarchy_pooling: true
  pooling_sizes: [4, 4, 2, 2, 1, 1]
  # pooling_sizes: [4, 4, 4, 4, 4, 4]

training:
  notes: "Efficient_Transformer_V4_HierarchyPool=421"

